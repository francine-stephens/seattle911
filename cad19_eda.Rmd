---
title: "Seattle 911 EDA"
author: "Francine Stephens"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    theme: united
    highlight: tango
---


This web report includes descriptive statistics of the Seattle 911 CAD data. The report starts with an overall summary of the structure of the dataset and then steps through each variable in the dataset.

```{r setup, include=F, warning=F, message=F}
knitr::opts_chunk$set(echo=T)

## Libraries
options(tidyverse.quiet = T)
packages <- c(
              "tidyverse",
              "scales",
              "naniar",
              "lubridate",
              "sf",
              "sp",
              "ggplot2",
              "ggmap",
              "maptools",
              "raster",
              "spatstat",
              "plotly",
              "tigris",
              "leaflet",
              "RColorBrewer", 
              "censusapi", 
              "tidycensus", 
              "openxlsx"
              )
lapply(packages, library, character.only = T)
options(dplyr.summarise.inform = F)

## Local Paths 
setwd("~/Projects/seattle911")
wd <- getwd()
shp_path <- "/shp"
precs_path <- "/Spd_Precincts/"

## API
register_google(key = "AIzaSyCztwSCIHyEKP_uyjPrdZKf3jgZFA-XV1Y",
                write = T)

```

## Dataset Description

Let's start by identifying the dimensions in the dataset. 
```{r data import, warning=F, message=F}
## Import Data & Set Variable Names + Types
cad <- 
  read_csv("2019 CAD Data.csv", 
           col_types = cols(.default = "f", 
                            `CAD Event ID` = "d",
                            `Event First Dispatch Time (ATTR)` = "c",
                            `Dispatch Address` = "c",
                            `Officer Serial Num` = "c",
                            `Squad Desc` = "c",
                            `Dispatch Blurred Latitude` = "d",
                            `Dispatch Blurred Longitude` = "d",
                            `CAD Event Response Time (Seconds) (SUM)` = "d",
                            `Total Service Time (Seconds) (SUM)` = "d")
           )

cad <- cad %>% 
  select_all(funs(gsub(" ", "_", .))) %>%
  select_all(funs(gsub("[()]", "", .)))
saveRDS(cad, "cad_2019_data.rds")

## Number of events & variables
dim(cad)
```

There are 752,421 events and and 17 variables in the data. The variable names from the CAD export are listed below. 

```{r variables}
## View variable names
names(cad)
```


Now, let's find the number of categories in the categorical variables. In subsequent sections, I will step through each variable and summarize the distributions in greater detail. 

```{r levels of cat vars}
## Number of categories for categorical variables
sapply(cad[,sapply(cad, is.factor)], nlevels)
```

**Dispatch ID** - This is some sort of identifier. It's interesting that the identifiers are not unique to each event. What does the dispatch ID identify? Is this identifier going to be relevant for our analysis? 

**Call priority codes and Call type description** have a manageable number of categories - 9 and 8, respectively. After taking a deeper dive into the univariate statistics in the sections below and understanding what these categories mean, we can decide whether any of these categories should be aggregated. 

**Case Type Final and Case Type Initial Descriptions** - These two variables have the greatest number of categories with 343 and 235 categories, respectively. We will want to parse out the categories and see how to regroup into a smaller, more manageable set of categories for analysis. After looking over the categories we can figure out some strategies for aggregating categories. 

**Clear by description** - There are 23 categories in this variable. After further review below, we can look to see if any aggregation is necessary. 

**Precinct** is a categorical spatial indicator. It looks like the city is divided into 6 regional precincts. 

**Sector** - There are 17 sectors. This variable appears to be another spatial category related to precinct. This will be described in the sector section below. 


Before diving into the distributions of the categorical variables in greater detail, let's take advantage of the fact that the data are time-stamped and get a sense of the frequency of events throughout the year.


## Event Dates & Times

The data are time stamped to the minute. In the graph below, I have displayed the frequency of events per day. Hover your mouse over the line graph to see the number of events that occurred on a given day. 

```{r event time graph}
## Event First Dispatch Time - Date Formats
cad <- cad %>% 
  mutate(event_datetime = mdy_hm(Event_First_Dispatch_Time_ATTR),
         event_date = as.Date(event_datetime),
         event_month = month(event_datetime, label=T)
         )  # set as date-time

# Graph events over time
event_time_line <- cad %>%
  group_by(event_date) %>%
  summarize(freq = n()) %>%
  ggplot(., aes(x=event_date, y=freq)) +
  geom_line(color="steelblue") + 
  labs(title = "Frequency of Events in 2019", 
       subtitle = "Seattle, WA", 
       y = "Number of Events", 
       x = "Time") +
  scale_y_continuous(labels = scales::comma, breaks=seq(0,2600,500)) + 
  theme(axis.text.x=element_text(angle=60, hjust=1)) + 
  scale_x_date(limit=c(as.Date("2019-01-01"),as.Date("2019-12-31")),
                 date_labels = "%b", date_minor_breaks = "1 month") + 
  theme_classic()

event_time_line %>%
  ggplotly() %>%
  layout(
    xaxis = list(fixedrange=T),
    yaxis = list(fixedrange = T)
  ) %>%
  config(displayModeBar = F)

```

The date with the highest number of events recorded was 2,530, which was on July 13th. In general, the summer months appear to have higher frequencies that the rest of the year. 

November 14th, 2019 is the date with the most marked decrease in events. There were only 76 events recorded on November 14th. This is far below other days with fewer events than normal, as shown in Table 1 below. *It raises the possibility of a glitch in the reporting system for that day.*


```{r event date table, warning=F, message=F}
## Table of Event date frequency
knitr::kable(cad %>%
  group_by(event_date) %>%
  summarize(freq = n()) %>%
  mutate(rank = min_rank(-freq)) %>%
  arrange(-freq) %>% 
  slice(1:10, 356:365),
  caption = "Table 1: Dates with Highest and Fewest Events",
  col.names = c("Date", "#", "Rank"), 
  format.args = list(big.mark = ",")
  )

```

On average, there were 2,061 events per day in 2019. With the exception of the 76 event day on November 14th, there is not much skewedness in the distribution of events throughout the year. 

```{r summary events}
knitr::kable(cad %>%
  group_by(event_date) %>%
  summarize(freq = n()) %>%
  summarize(avg = mean(freq, na.rm = T),
            sd = sd(freq, na.rm = T),
            med = median(freq, na.rm = T)), 
  caption = "Table 2: Events Over Time Summary",
  col.names = c("Daily Avg", "Std. Dev", "Median"), 
  format.args = list(big.mark = ",")
)
```


## Call Priority Codes

```{r call code bar graph}
## Call Priority Codes
cad <- cad %>%
  mutate(Call_Priority_Code = factor(Call_Priority_Code,
                                     levels = c("-1",
                                                "1",
                                                "2",
                                                "3",
                                                "4",
                                                "5",
                                                "6",
                                                "7",
                                                "8",
                                                "9")))

call_priority_bar <- cad %>%
  group_by(Call_Priority_Code) %>%
  summarize(freq = n()) %>%
  ggplot(.,
       aes(x=Call_Priority_Code, y=freq, fill=Call_Priority_Code)) + 
  geom_bar(stat="identity") +
  scale_fill_brewer(palette = "Set1") + 
  labs(title = "Frequency of Call Priority Codes",
  subtitle = "2019 Events", y = "Number of Events", x = "Priority Code") +
  scale_y_continuous(labels = scales::comma) + 
  theme_minimal() + theme(legend.position="none")

call_priority_bar %>%
  ggplotly() %>%
  layout(
    xaxis = list(fixedrange = T),
    yaxis = list(fixedrange = T)
  ) %>%
  config(displayModeBar = F)

```

Code 2 is the most common priority code recorded with a total of 219,406 in 2019. According to Table 3,  Code 2 is about 30% of the events in 2019. Just over three-quarters of the events are categorized as being categorized as priority codes 1 through 3. 

Codes 6 and -1 have the fewest events. They do not show up as clearly in the graph, but in Table 3, shown below, they total to 44 and 274 events, respectively. I dug into the -1 code a little bit more and it looks like this code is applied to very specific cases. *All 274 of these events had the call type listed as Onview and had the initial case type description of "DOWN - CHECK FOR DOWN PERSON".* You can flip through the paged table below to see the events with -1 priority codes. 

One other point to note is that there is not a code 8; the codes skip from 7 to 9.

```{r call priority table}
## Table of call priority
knitr::kable(cad %>% 
               group_by(Call_Priority_Code) %>%
               summarize(freq = n()) %>%
               mutate(percent = round((freq/sum(freq)) * 100, digits=2), 
                      freq = scales::comma(freq)),
             caption = "Table 3: Call Priority Codes",
             col.names = c("Code", "# Events", "%"), 
             format.args = list(big.mark = ",")
             )

```

**Table of -1 Priority Code Events**
```{r view code -1}
# Identify the -1 events 
neg_priority_code <- cad %>%
  filter(Call_Priority_Code == "-1") %>%
  select(event_datetime,
         Call_Type_Desc,
         Case_Type_Final_Desc,
         Case_Type_Initial_Desc)


rmarkdown::paged_table(neg_priority_code)
```


## Call Type Description
```{r call type description}
## Call Type Descriptions
knitr::kable(cad %>%
  group_by(Call_Type_Desc) %>% 
  summarize(freq = n()) %>%
  mutate(percent = round((freq/sum(freq)) * 100, digits=2),
         freq = scales::comma(freq)
         ) %>%
  arrange(-percent),
  caption = "Table 4: Call Type Description",
             col.names = c("Type", "# Events", "%"), 
             format.args = list(big.mark = ",")
)

```

911 calls are about 43% of the events. Onview and some other telephone call are the second and third most common types, and together they comprise just over 50% of the event types. Text message, officer initiated, scheduled recurring event, and in-person complaints are very minimal sources of the events. 

**Questions**
1) Is the plan to focus solely on 911 call types? (If so, the below questions are not relevant.)
2) What is the difference between Onview and Proactive(Officer Iniatied)? Is it possible that these two types would be worth combining?
3) Bivariate/Spatial relationships may be interesting to investigate, e.g., Are Onview and Proactive types more common in certain neighborhoods? Where are other telephone, not 911 calls most prevalent? What case types are most prevalent among the four least common call types?


## Case Type Final Description
```{r final case type}
## Case type Final categories
case_type_final_desc <- cad %>%
  group_by(Case_Type_Final_Desc) %>%
  summarize(freq = n()) %>% 
  mutate(percent = round((freq/sum(freq)) * 100, digits=2),
         freq = scales::comma(freq),
         Case_Type_Final_Desc = as.character(Case_Type_Final_Desc)
         ) %>%
  arrange(Case_Type_Final_Desc)

rmarkdown::paged_table(case_type_final_desc %>%
  arrange(-percent)
)

```

Flip through the pages in the table to view the number of events with each type of case final description. Recall that this variable has 343 different descriptions. 

Some of these descriptions have a general description followed by a more specific description that follows a dash. We could parse on the general description and then aggregate to get a smaller set of categories. I demonstrate this in the table below. 

```{r final case description aggregation}
## Aggregate final case description
cad <- cad %>%
  mutate(
    case_type_final_desc_first = str_trim(str_extract(Case_Type_Final_Desc, "[^-]+"),
                                          side = c("both")),
    case_type_final_desc_second = str_trim(str_extract(Case_Type_Final_Desc, "[^-]*$"),
                                           side = c("both"))
  )

case_type_final_before_dash <- cad %>%
  group_by(case_type_final_desc_first) %>%
  summarize(freq = n()) %>% 
  mutate(percent = round((freq/sum(freq)) * 100, digits=2),
         freq = scales::comma(freq),
         case_type_final_desc_first = as.character(case_type_final_desc_first)
         ) %>%
  arrange(case_type_final_desc_first)

case_type_final_after_dash <- cad %>%
  group_by(case_type_final_desc_second) %>%
  summarize(freq = n()) %>% 
  mutate(percent = round((freq/sum(freq)) * 100, digits=2),
         freq = scales::comma(freq),
         case_type_final_desc_second = as.character(case_type_final_desc_second)
         ) %>%
  arrange(case_type_final_desc_second)

# Table of case type description before the dash 
rmarkdown::paged_table(case_type_final_before_dash %>%
  arrange(-percent)
)

```

This aggregation strategy reduced the number of categories to 153. Traffic related cases are the most common followed by disturbance and suspicious circumstances. If you flip through the pages, there are some categories that also appear to be similar to these top 3. For instance, traffic stop is listed on page 6, which seems like it could also fit under traffic. Also on page 6 is the category suspicious stop, which seems related to suspicious circumstances. All of descriptions and frequencies for the final case type descriptions are listed in the exported Excel file. 

*Other Comments*
* Need to make sure to catch abbreviations using reg. expressions (e.g., burg --> burglary)
* Similarly, use reg. expressions for categories that look alike but differ in terms of spacing (e.g., Arson, Bombs, Explo; Abandoned car & Abandoned vehicle)
* "#NAME?" looks like it might be the classification for events that were not classified. There are 19,900 events with this classification, which is about 2.64 events.

```{r final case descriptions export}
# Write Final Case Description Data Frames to Excel Workbook
case_descriptions_workbook <- createWorkbook()

addWorksheet(case_descriptions_workbook, sheetName="full_final_desc")
addWorksheet(case_descriptions_workbook, sheetName="first_final_desc")
addWorksheet(case_descriptions_workbook, sheetName="second_final_desc")

writeData(case_descriptions_workbook, "full_final_desc", case_type_final_desc)
writeData(case_descriptions_workbook, "first_final_desc", case_type_final_before_dash)
writeData(case_descriptions_workbook, "second_final_desc", case_type_final_after_dash)

```


## Case Type Initial Descriptions
```{r initial case type}
## Case type Initial categories
case_type_init_desc <- cad %>%
  group_by(Case_Type_Initial_Desc) %>%
  summarize(freq = n()) %>% 
  mutate(percent = round((freq/sum(freq)) * 100, digits=2),
         freq = scales::comma(freq),
         Case_Type_Initial_Desc = as.character(Case_Type_Initial_Desc)
         ) %>%
  arrange(Case_Type_Initial_Desc)

rmarkdown::paged_table(case_type_init_desc %>%
  arrange(-percent)
)

```
The top three/four initial case type descriptions occur at about the same frequency. The top four are also in the top four in the final description, but the ordering differs.

One note on structure of these descriptions is that not as many of these descriptions have the same structure as noted in the final descriptions, that is a general description followed by a more specific description/detail, with the two descriptions separated by a dash "-". Below, I have parsed out the description as I did with the final case descriptions, however, it may be a less useful approach for this description. 


*Other Comments/Questions*
* Need to make sure to catch abbreviations using reg. expressions (e.g., HAZ --> HAZARD)
* "#NAME?" shows up again in this set of descriptions, though not as frequently as it did in the final descriptions (n=12,132). 
* Would it be useful to compare final and initial descriptions? We could use some fuzzy matching and regular expressions if this is something important. If final descriptions are missing (meaning that they are coded as #NAME?) and initial descriptions are not missing, should the initial description be applied?

```{r initial case description aggregation}
## Aggregate initial case description
cad <- cad %>%
  mutate(
    case_type_initial_desc_first = str_trim(str_extract(Case_Type_Initial_Desc, "[^-]+"),
                                          side = c("both")),
    case_type_initial_desc_second = str_trim(str_extract(Case_Type_Initial_Desc, "[^-]*$"),
                                           side = c("both"))
  )

case_type_init_before_dash <- cad %>%
  group_by(case_type_initial_desc_first) %>%
  summarize(freq = n()) %>% 
  mutate(percent = round((freq/sum(freq)) * 100, digits=2),
         freq = scales::comma(freq),
         case_type_initial_desc_first = as.character(case_type_initial_desc_first)
         ) %>%
  arrange(case_type_initial_desc_first)

case_type_init_after_dash <- cad %>%
  group_by(case_type_initial_desc_second) %>%
  summarize(freq = n()) %>% 
  mutate(percent = round((freq/sum(freq)) * 100, digits=2),
         freq = scales::comma(freq),
         case_type_initial_desc_second = as.character(case_type_initial_desc_second)
         ) %>%
  arrange(case_type_initial_desc_second)

# Table of case type description before the dash 
rmarkdown::paged_table(case_type_init_before_dash %>%
  arrange(-percent)
)

```

Aggregating reduced the number of descriptions down to 125. The top four descriptions remain the same, but the rest of the top 10 have shifted ranks (e.g., assault, trespass). 

```{r initial case descriptions export}
# Write Initial Case Description Data Frames to Excel Workbook & Export
addWorksheet(case_descriptions_workbook, sheetName="full_init_desc")
addWorksheet(case_descriptions_workbook, sheetName="first_init_desc")
addWorksheet(case_descriptions_workbook, sheetName="second_init_desc")

writeData(case_descriptions_workbook, "full_init_desc", case_type_init_desc)
writeData(case_descriptions_workbook, "first_init_desc", case_type_init_before_dash)
writeData(case_descriptions_workbook, "second_init_desc", case_type_init_after_dash)

saveWorkbook(case_descriptions_workbook,
             file= "CAD19_case_descriptions.xlsx",
             overwrite = T)

```


## Clear by Description
```{r clear by desc}
knitr::kable(cad %>% 
group_by(Clear_By_Desc) %>% 
  summarize(freq = n()) %>% 
  mutate(percent = round((freq/sum(freq)) * 100, digits=2),
         freq = scales::comma(freq)) %>%
  arrange(-percent),
  caption = "Table 4: Clear By Descriptions",
  col.names = c("Description", "# Events", "%"), 
  format.args = list(big.mark = ",")
)

```

* It looks like a dash "-" represents missing clear by description (n=2,626). 
* There are some descriptions that I do not know what they mean or how they differ from other descriptions. For instance, how are responding units canceled by radio and duplicated or canceled by radio different?
* Unable to locate incident or complainant is about 7.7% of the events. 


## Precinct & Sector
```{r precinct table}
## Precinct Table
knitr::kable(cad %>% 
group_by(Precinct) %>% 
  summarize(freq = n()) %>% 
  mutate(percent = round((freq/sum(freq)) * 100, digits=2),
         freq = scales::comma(freq)) %>%
  arrange(-percent),
  caption = "Table 5: Events by Precinct",
  col.names = c("Precinct", "# Events", "%"), 
  format.args = list(big.mark = ",")
)

```

```{r precinct bar graph}
## Distribution of Events by Precinct
precinct_freq_bar <- cad %>%
  group_by(Precinct) %>% 
  summarize(freq = n()) %>%
  mutate(percent = round((freq/sum(freq)) * 100, digits=2)) %>%
  ggplot(.,
       aes(x = reorder(Precinct, -freq), y = freq, fill = Precinct)) + 
  geom_bar(stat="identity") +
  scale_fill_brewer(palette = "Set1") + 
  labs(title = "Events per Precinct",
  subtitle = "2019 Events", y = "Number of Events", x = "Precinct") +
  scale_y_continuous(labels = scales::comma) + 
  theme_minimal() + theme(legend.position = "none")

precinct_freq_bar %>%
  ggplotly() %>%
  layout(
    xaxis = list(fixedrange = T),
    yaxis = list(fixedrange = T)
  ) %>%
  config(displayModeBar = F)

```


The western precinct had the most events in 2019, with about 29% of the events occurring in the precinct. North was the next common, comprising about 25% of the events. Southwest had the fewest number of events recorded with 84,140 events last year. 

For 5,932 of the events, the precinct is unknown. We may be able to identify a precinct for these events if they have valid latitude and longitude coordinates. Let's look to see if they do have lat and long: 

```{r coordinates for unknown precincts}
## How many events with unknown precincts have valid coordinates?
knitr::kable(cad %>%
  filter(Precinct == "UNKNOWN") %>%
  mutate(coords_status = as.factor(if_else(
    Dispatch_Blurred_Longitude <-121 & Dispatch_Blurred_Latitude > 46,
                                    1,
                                    0)),
    coords_status = recode_factor(coords_status,
                                 "0" = "Not valid coords",
                                 "1" = "Valid coords")) %>%
  group_by(coords_status) %>%
  summarize(unknown_precincts_coord_freq = n()),
  caption = "Table 6: Unknown Precinct Coordinate Status",
  col.names = c("Coordinate Status", "# Events"), 
  format.args = list(big.mark = ",")
)
  
```

The majority of the events with unknown precincts do not have coordinates that are within the extent of Seattle/King County, Washington. We can however use 1,962 of these events with unknown precincts as they do have coordinates that fall within the geographic extent of Seattle. When I create a spatial object from the coordinates, as shown a few sections below, I will be able to plot these. For some it may be obvious what the precinct is based on the precinct labels given to neighboring events. If the precinct classification is not obvious, the best thing to do would be to obtain a shapefile of the polygons for each of the five precincts, overlay it on the events and give the point the name of the polygon precinct that it falls within. [Seattle's Open Data website](https://data.seattle.gov/Public-Safety/Spd-Precincts/3rtr-jjhz) has such a shapefile that I will call on and use in the spatial geoprocessing section below. 

There are some interesting bivariate analyses that could be explored. For example, call priority codes and precincts. View the interactive stacked bar chart below.

```{r bivariate table prec call priority}
## Call Priority Codes & Precincts
call_priority_precincts_stack <- cad %>% 
  mutate(Call_Priority_Code = as.character(Call_Priority_Code),
         Call_Priority_Code = paste0("Code ", Call_Priority_Code),
         Call_Priority_Code = as.factor(Call_Priority_Code)) %>%
  group_by(Precinct, Call_Priority_Code) %>%
  summarize(freq = n()) %>%
  ggplot(., aes(fill=Precinct, y=freq, x=Call_Priority_Code)) + 
  geom_bar(position="fill", stat="identity") +
  scale_fill_brewer(palette = "Set2") +
  labs(
    x = "Call Priority Code",
    y = "Proportion of Events",
    title = "Case Priority Code by Precinct",
    subtitle = "Seattle, WA",
    fill = "Precinct",
    caption = "Seattle, WA 2019"
  ) +
  coord_flip() + 
  theme_classic() +
  theme(
    legend.position = "bottom",
    legend.direction = "vertical",
    axis.text.x = element_text( hjust=1)
    )
  
call_priority_precincts_stack %>%
  ggplotly() %>%
  layout(
    xaxis = list(fixedrange = T),
    yaxis = list(fixedrange = T)
  ) %>%
  config(displayModeBar = F)

```

A few things stand out in the stacked bar graph of call priority codes and precincts. 
* Just under half of the events with the specialized code -1 were in the Western precinct. 
* The North and West precincts had very similar shares of events in codes 1 through 3. In each of these codes, the cases in the North and West total just over 50% of the cases with that code. * Just over 40% of the events classified as code 6 are in the Northern precinct. 
* About 45% of code 9 events are in the Western precinct, which is similar to the share of code -1 events. 


Let's turn to focus on the sectors. There are 17 distinct sector names. 5,932 events that were not given a sector. These events are identical to those missing a precinct classification.  

```{r sector and precinct graph}
## Sector by Precinct Graph
sector_precinct_freq_bar <- cad %>% 
  group_by(Precinct, Sector) %>% 
  summarize(freq = n()) %>% 
  mutate(percent = round((freq/sum(freq)) * 100, digits=2)) %>%
  ggplot(.,
       aes(x = reorder(Sector, -freq), y = freq, fill = Precinct)) + 
  geom_bar(stat="identity") +
  scale_fill_brewer(palette = "Set1") + 
  labs(title = "Events per Sector-Precinct",
  subtitle = "2019 Events", y = "Number of Events", x = "Sector") +
  scale_y_continuous(labels = scales::comma) + 
  theme_minimal() + 
  theme(legend.position = "bottom") + 
  theme(axis.text.x = element_text(angle = 90))

sector_precinct_freq_bar %>%
  ggplotly() %>%
  layout(
    xaxis = list(fixedrange = T),
    yaxis = list(fixedrange = T)
  ) %>%
  config(displayModeBar = F)

```

```{r sector and precinct table}
## Sector by Precinct Table
knitr::kable(cad %>% 
  group_by(Precinct, Sector) %>% 
  summarize(freq = n()) %>% 
  mutate(percent = round((freq/sum(freq)) * 100, digits=2),
         freq = scales::comma(freq)) %>%
  arrange(Precinct, -percent),
  caption = "Table 7: Events by Precinct-Sector",
  col.names = c("Precinct","Sector", "# Events", "Percent"), 
  format.args = list(big.mark = ",")
)

```
Sectors are unique to precincts. We can think of a sectors as a subdivision of the precinct. In the table above, we see that within the South precinct, Ocean sector had about 40% of the events, whereas the other two sectors - Robert and Sam - were about 30% each. 

The Edward sector had nearly half of the events in the East precinct. 

The Southwest precinct's events were relatively evenly divided among the two sectors - Frank and William.

The West precinct, which has the most events of all the precincts, has a wide spread in terms of the number of events in each of its four sectors. The King sector had the most events (76,274 about 35%), and Queen sector had the fewest (38,454 about 18%). 

The North precinct has 5 sectors. Boy, Nora, and Union sectors have similar shares of events within their boundaries. The other two sectors - Lincoln and John - make-up just over 30% of the events in the precinct. 

*The Seattle Open Data website does not appear to have a boundary shapefile or API for sector. This may be something to inquire about if we want to do point-in-polygon analyses.*


## Squad Description
```{r squad desc table}
## Squad Descriptions Frequencies
rmarkdown::paged_table(cad %>%
  group_by(Squad_Desc) %>%
  summarize(freq = n()) %>%
  arrange(-freq) %>%
  mutate(percent = round((freq/sum(freq)) * 100, digits=2),
         freq = scales::comma(freq))
)

```
This is one of the variables with an unmanageable amount of categories. There are only 2,746 events missing a squad description. If you flip through the pages of the table you can see that the squad groups are named in various ways. Some are based on the field/area they work in (e.g., forensics, Arson/Bomb) and others are based on locations (i.e., precinct + sector). **If this is a variable that is considered important we would need to approach the aggregation like we would for the Case type descriptions using the first descriptor before the dash, regular expressions, and lazy matching to get broad categories and abbreviations, misspellings, and differences in ordering of words. **


## Officer Identifier
```{r officer serial num}
## Number of officers in the dataset
n_distinct(cad$Officer_Serial_Num)

```
There are 1,342 officers in this dataset.


## Response Time
```{r response time sum stats}
## Response Time
summary(cad$CAD_Event_Response_Time_Seconds_SUM)

```

Response time for each event is reported in seconds. The summary statistics suggest that there are some very long response times that are outliers. The longest response time is 14,773,646 seconds, which would be many, many days long. Let's parse the seconds into higher levels of time. 

```{r response time converted to period}
## Response Time parsed into periods
cad <- cad %>%
  mutate(response_time_parsed = seconds_to_period(CAD_Event_Response_Time_Seconds_SUM)) 

# Long to Short Response Time
rmarkdown::paged_table(cad %>%
  select(event_date, response_time_parsed, Case_Type_Final_Desc) %>%
  arrange(-response_time_parsed)
)
  
```

With the times parsed into periods and sorted from longest to shortest time, we can see that the longest time was 170 days and the case was a test call. This is probably a candidate for excluding. For completeness, below the data displayed sorted from shortest to longest, so that it is easier to see what the short response times are.

```{r short to long response time}
# Short to Long Response Time
rmarkdown::paged_table(cad %>%
  select(event_date, response_time_parsed, Case_Type_Final_Desc) %>%
  arrange(response_time_parsed)
)
  
```

## Total Service Time
```{r total service time}
summary(cad$Total_Service_Time_Seconds_SUM)

```

The distribution for total service time on events is strange. There are 281 events missing a total service time. Additionally, there is at least one event that had a negative total service time recorded. First, let's see how many negative values we have. 


```{r total service time negs}
## Total Service Time Negative Values
knitr::kable(cad %>%
  filter(Total_Service_Time_Seconds_SUM <0) %>%
  select(event_date, Total_Service_Time_Seconds_SUM, response_time_parsed, Case_Type_Final_Desc) %>%
  arrange(Total_Service_Time_Seconds_SUM),
  caption = "Table 7: Total Service Time",
  col.names = c("Date", "Service Time (Seconds)", "response time parsed", "Case Type Final"), 
  format.args = list(big.mark = ",")
)

```

There are only 8 events in the dataset with negative values. When we include information like the event date, parsed response time, and case description type, we notice that two of these are duplicates. The other thing that stands out is that these events were all recorded on the same date, November 3rd. It is possible that the negative values were a recording error that occurred that day. We could also check for the average service time on other events of a similar type to see if the absolute value of total service time is reasonable. 

Now, let's look at the NA values. 
```{r total service time NAs}
## Total Service Time NAs
rmarkdown::paged_table(cad %>%
  filter(is.na(Total_Service_Time_Seconds_SUM)) %>%
  select(event_date, Total_Service_Time_Seconds_SUM, response_time_parsed, Case_Type_Final_Desc) %>%
  arrange(event_date)
)

```
The events with missing values vary on case types. There appears to be some duplicates, e.g., the assault-DV case on January 13th. Again, it seems like event date and response time would be useful for identifying duplicates and then de-duplicating this dataset. 

For the sake of consistency, I parsed the total service time into time periods as I did with the response time. See some of the output below. 

```{r total service time parsed}
## Service Time parsed into periods
cad <- cad %>%
  mutate(total_service_time_parsed = seconds_to_period(Total_Service_Time_Seconds_SUM)) 

# Long to Short Response Time
rmarkdown::paged_table(cad %>%
  select(event_date,
         total_service_time_parsed,
         response_time_parsed,
         Case_Type_Final_Desc, 
         Case_Type_Initial_Desc) %>%
  arrange(-total_service_time_parsed)
)

```

With the parsed by period version of service time, we see that the upper end of the service time distribution is 2 days. Notice that the six longest entries are missing case descriptions. There is also a duplicate among these six - the one on March 23rd. If you flip through the pages, you can spot more duplicates, which again suggests that de-duplicating on event date, time, and case type description would be most useful.

## Spatial Object

Before transforming the dataframe into a spatial object, the calls with missing or invalid coordinates need to be removed. After filtering those calls out, the transformed spatial object contains 675,664 call locations. In total there are 76,757 events that do not have valid coordinates. 

```{r spatial object}
## Transform to Spatial Object
cad_sf <- cad %>% 
  filter(Dispatch_Blurred_Longitude <-121 & Dispatch_Blurred_Latitude > 46) %>%  
  st_as_sf(coords=c("Dispatch_Blurred_Longitude", "Dispatch_Blurred_Latitude"),
           crs=4326)
```

Mapping all of these calls as points would result in over-plotting. There are other approaches for visualizations that would be more informative. One approach is to aggregate the points to meaningful geographic units like zipcodes or neighborhoods. Another approach is to create a point density map to show where the highest and lowest number of events per area occurred in the city. The following sections demonstrate these approaches.

Let's start by visualizing the frequency of events in different geographic regions of Seattle. In one of the prior sections, I showed the frequency of events per precinct. However, approximately 5,000 of the calls did not have a precinct listed. Now that the dataframe has been transformed to a spatial object, I can identify a precinct for those locations based on which precinct each coordinate pair lies within. 

## Precinct Aggregation & Mapping
```{r events per precinct}
## Geoprocess precincts
precincts_sf <- st_read(paste0(wd,
                               shp_path, 
                               precs_path, 
                          "geo_export_8e9605ae-b15a-49cc-bcbd-73d6fabe122e.shp")
                        )

precincts_sf <- precincts_sf %>%
    st_transform(., crs = st_crs(cad_sf))
 
## Events per precinct
events_per_precinct <- cad_sf %>%
  st_set_crs(., st_crs(precincts_sf)) %>%
  st_join(.,
          precincts_sf,
          join = st_within) %>%
  group_by(name) %>%
  summarize(events = n())  %>%
  st_set_geometry(NULL) %>%
  mutate(precinct = recode_factor(name,
                         "E" = "EAST",
                         "N" = "NORTH",
                         "S" = "SOUTH",
                         "SW" = "SOUTHWEST",
                         "W" = "WEST"
                         )
         )

knitr::kable(events_per_precinct %>%
               select(-name) %>%
               relocate(precinct, events) %>%
               arrange(-events) %>%
               mutate(events = scales::comma(events)), 
             caption = "Events per precinct, spatial overlay version",
             col.names = c("Precinct", "Events")
)

```

A couple of things standout from using the spatial overlay approach to assign precincts. First, the number of points that are not assigned to a precinct decreased from approximately 5,000 to 2,196. The reason these points are not assigned is because they lie outside of the precinct boundaries. 

However, what is also noteworthy is that the number of events per precinct are lower than they were in the section above where I used the precinct given in the data. This indicates that the over 70,000 events that were dropped due to invalid geometries had been given precinct identifiers, but not latitude and longitudes. If we want to preserve all of these points, the best thing to do would be to keep the precincts that were provided in the original dataset and then merge in the spatial overlay precincts for the subset of events that did not have valid coordinates. Let's do that and then visualize the precincts and frequencies now.


```{r calls per precinct mapped}
## Combine precinct assignments - Seattle PD classified & spatial overlay
events_precincts <- cad_sf %>%
  st_set_crs(., st_crs(precincts_sf)) %>%
  st_join(.,
          precincts_sf,
          join = st_within) %>% 
  st_set_geometry(NULL) %>%
  mutate(precinct = recode_factor(name,
                         "E" = "EAST",
                         "N" = "NORTH",
                         "S" = "SOUTH",
                         "SW" = "SOUTHWEST",
                         "W" = "WEST"
                         ),
         precinct = coalesce(precinct, Precinct)
         ) %>%
  select(-descriptio) %>% 
  group_by(precinct) %>%
  summarize(events = n())

invalid_geoms_precincts <- cad %>%
  filter(Dispatch_Blurred_Longitude >-121 & Dispatch_Blurred_Latitude < 46) %>%
  group_by(Precinct) %>% 
  summarize(Events = n()) 

merged_events_per_precinct_sf <- precincts_sf %>%
  mutate(Precinct = recode_factor(name,
                         "E" = "EAST",
                         "N" = "NORTH",
                         "S" = "SOUTH",
                         "SW" = "SOUTHWEST",
                         "W" = "WEST"
                         )) %>%
  left_join(., invalid_geoms_precincts, by = c("Precinct")) %>%
  left_join(., events_precincts, by = c("Precinct" = "precinct")) %>%
  mutate(total_events = (Events + events))

## Extract points outside of precinct boundaries
outside_precincts_points <- cad_sf[!lengths(st_intersects(cad_sf, precincts_sf)), ]

## Visualize 2019 events per precinct
prec_pal <- colorNumeric(
  palette = "Purples",
  domain = merged_events_per_precinct_sf$total_events
)

leaflet() %>%
  addTiles() %>% 
  addPolygons(
    data = merged_events_per_precinct_sf %>% st_transform(crs = 4326),
    fillColor = ~prec_pal(total_events),
    color = "Black",
    opacity = 0.5,
    fillOpacity = 0.5,
    weight = 1.5,
    label = ~Precinct) %>% 
  addCircles(
    data = outside_precincts_points,
    color = 'orange',
    fill = T,
    radius = 1,
    label = ~Case_Type_Final_Desc, 
  ) %>%
  addLegend(
    data = merged_events_per_precinct_sf,
    pal = prec_pal,
    values = ~total_events,
    title = "Map 1: 2019 Events<br>per Precinct"
  )

```


The map shows not only the events per precinct, but also those events that are outside of the precinct boundaries. 

## Zipcode Aggregation & Mapping
Another aggregation we can perform and visualize is at the zip-code level.

```{r aggregate to zipcodes}
## Zip code aggregation
king_zips_sf <- 
  st_read(
    "https://opendata.arcgis.com/datasets/83fc2e72903343aabff6de8cb445b81c_2.geojson"
    )

## Events per zip
events_per_zip <- cad_sf %>%
  st_join(.,
          king_zips_sf,
          join = st_within) %>%
  st_set_geometry(NULL) %>%
  group_by(ZIPCODE) %>%
  summarize(events = n())

events_per_zip_sf <- king_zips_sf %>%
  left_join(., events_per_zip, by = c("ZIPCODE")) %>%
  filter(!is.na(events))

knitr::kable(events_per_zip_sf %>%
  select(ZIPCODE, events) %>% 
    st_set_geometry(NULL) %>%
  arrange(-events) %>% 
    mutate(events = scales::comma(events)), 
  caption = "Events per zipcode",
             col.names = c("Zipcode", "Events")
)

```

```{r visualize zips}
## Visualize 2019 events per zip
zips_pal <- colorNumeric(
  palette = "Reds",
  domain = events_per_zip_sf$events
)

leaflet() %>%
  addTiles() %>% 
  addPolygons(
    data = events_per_zip_sf,
    fillColor = ~zips_pal(events),
    color = "Black",
    opacity = 0.5,
    fillOpacity = 0.5,
    weight = 1.5,
    label = ~ZIPCODE) %>% 
  addLegend(
    data = events_per_zip_sf,
    pal = zips_pal,
    values = ~events,
    title = "Map 2: 2019 Events<br>per Zipcode"
  )

```

## Neighborhood Aggregation & Mapping
```{r aggregate nhoods}
nhoods_sod_sf <- 
  st_read(
    "https://opendata.arcgis.com/datasets/b76cdd45f7b54f2a96c5e97f2dda3408_2.geojson"
    )

## Events per zip
events_per_nhood <- cad_sf %>%
  st_join(.,
          nhoods_sod_sf,
          join = st_within) %>%
  st_set_geometry(NULL) %>%
  group_by(S_HOOD) %>%
  summarize(events = n())

events_per_nhood_sf <- nhoods_sod_sf %>%
  left_join(., events_per_nhood, by = c("S_HOOD")) %>%
  filter(!is.na(events))

knitr::kable(events_per_nhood_sf %>%
  select(S_HOOD, events) %>% 
    st_set_geometry(NULL) %>%
  arrange(-events) %>% 
    mutate(events = scales::comma(events)), 
  caption = "Events per neighborhood",
             col.names = c("Nhood", "Events")
)

```

```{r nhood visualize}
nhoods_pal <- colorNumeric(
  palette = "Oranges",
  domain = events_per_nhood_sf$events
)

leaflet() %>%
  addTiles() %>% 
  addPolygons(
    data = events_per_nhood_sf,
    fillColor = ~nhoods_pal(events),
    color = "Black",
    opacity = 0.5,
    fillOpacity = 0.5,
    weight = 1.5,
    label = ~S_HOOD) %>% 
  addLegend(
    data = events_per_nhood_sf,
    pal = nhoods_pal,
    values = ~events,
    title = "Map 3: 2019 Events<br>per Neighborhood"
  )

```


## Point Density Mapping
```{r map by call codes}
# Create Seattle boundary and transform to sp 
seattle_sf <- events_per_zip_sf %>%
  mutate(city = "Seattle") %>%
  group_by(city) %>%
  summarize() 

seattle_sp <- seattle_sf %>% 
  st_transform(., crs=2285) %>%
  as(., "Spatial")

seattle_Owin <- as.owin(seattle_sp)


 # Load a starbucks.shp point feature shapefile
cad_sp <- cad_sf %>%
  st_transform(., crs=2285) %>%
  as(., "Spatial")
cad_ppp  <- as.ppp.SpatialPointsDataFrame(cad_sp)
marks(cad_ppp) <- NULL
Window(cad_ppp) <- seattle_Owin


# rast_seattle <- raster(seattle_sp)
# rast_seattle <- rasterize(seattle_sp, rast_seattle)
# quads_seattle <- as(rast_seattle, 'SpatialPolygons')
# plot(quads_seattle, add=T)
# points(cad_sp, col='red', cex=.5)

dens_cad <- density(cad_ppp)
plot(dens_cad, main='911 Event Density, Seattle, WA 2019')


ggplot(cad_sp, aes(x = lon, y = lat)) + 
  coord_equal() + 
  xlab('Longitude') + 
  ylab('Latitude') + 
  stat_density2d(aes(fill = ..level..), alpha = .5,
                 geom = "polygon", data = crime) + 
  scale_fill_viridis_c() + 
  theme(legend.position = 'none')

```

```{r points}
## Ggplot version
ggmap(seattle_base, extent ="panel", base_layer = ggplot(cad_valid_coords, aes(x=Dispatch_Blurred_Longitude, y=Dispatch_Blurred_Latitude))) +
  coord_equal() + 
  xlab('Longitude') + 
  ylab('Latitude') + 
  stat_density2d(geom = "point",
  aes(size = after_stat(density)),
  contour = T) + 
  scale_fill_viridis_c() #+ 
  #theme(legend.position = 'none')
    

```


```{r full points}
seattle_base <- get_googlemap(
  "Seattle, Washington", 
  zoom = 11,
  maptype = "roadmap")

cad_valid_coords <- cad %>%
  filter(Dispatch_Blurred_Longitude <-121 & Dispatch_Blurred_Latitude > 46) 


  ggmap(seattle_base, extent ="panel") +
  geom_point(data = cad_valid_coords, aes(x=Dispatch_Blurred_Longitude, y=Dispatch_Blurred_Latitude), size = 0.1, alpha = 0.05) + 
    coord_equal() +
  xlab('Longitude') + 
  ylab('Latitude')


```